{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L2H_ER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4YB7yq9bqLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21add501-e98b-4118-c56d-67ca02e1c37f"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ96cjjYDtoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_nas-FhcoUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b143319e-7818-49af-d27b-b89c40e78b5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive/')\n",
        "root_path = 'gdrive/My Drive/hashes'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at gdrive/; to attempt to forcibly remount, call drive.mount(\"gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDaQDHwUDz6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensorboard:\n",
        "    \"\"\"\n",
        "    Custom Tensorboard to post experiment summaries.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logdir):\n",
        "        self.writer = tf.compat.v1.summary.FileWriter(logdir)\n",
        "\n",
        "    def close(self):\n",
        "        self.writer.close()\n",
        "\n",
        "    def log_summary(self, hash_codes, coverage, computation, iteration):\n",
        "        summary = tf.Summary(value=[\n",
        "            tf.Summary.Value(tag='DeepHash/HashCodes',\n",
        "                             simple_value=hash_codes),\n",
        "            tf.Summary.Value(tag='DeepHash/Coverage',\n",
        "                             simple_value=coverage),\n",
        "            tf.Summary.Value(tag='DeepHash/Computation',\n",
        "                             simple_value=computation)])\n",
        "\n",
        "        self.writer.add_summary(summary, iteration)\n",
        "\n",
        "        self.writer.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YODkb2N3D5ON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b325c20a-d81d-4500-e49f-b19850fa2751"
      },
      "source": [
        "descriptor=\"_generated_hashes_codelength128_simweight_abitbetter_smaller_probs__probs_128_128\"\n",
        "tensorboard = Tensorboard(\"gdrive/My Drive/ERProject/DeepHash/\"+descriptor)\n",
        "slim = tf.contrib.slim\n",
        "row_size=9004# size of the dimension we are limiting our embedding  \n",
        "code_length=16 #6#16# can be modified depending upon computation and coverage\n",
        "external_dropout=0.8 #Percentage of things that will remain\n",
        "internal_dropout=0.65 #Percentage of things that will remain\n",
        "optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
        "batch_size= 32\n",
        "total_it=10000#0000 #Total number of iterations, each iteration has a number of batches and gives a report at the end\n",
        "num_epochs=2 #Epochs per batch\n",
        "report_interval=1000\n",
        "similarity_weight=1.0 #This is a hyper-parameter we can use to boost the importance of the similarity in the computation\n",
        "use_probs=True\n",
        "location=\"gdrive/My Drive/ERProject/Data/\"\n",
        "df = pd.read_csv(location+\"pagetitle_embedding_skigram_loss_0.40_200407.csv\", header=0)\n",
        "df2 = pd.read_csv(location+\"binary_codes.csv\", header=0, dtype={'300': str, '301': str})\n",
        "df2.drop(['spec_id','page_title','brand','model'], 1, inplace=True)\n",
        "#print(df2.head())\n",
        "#df2 = pd.read_csv(location+\"ProductIdsPageTitles.csv\", header=0)\n",
        "#df2.drop(df2.columns.difference(['spec_id']), 1, inplace=True)\n",
        "#df1.drop(columns=[100], inplace=True)\n",
        "df=  pd.concat([df.reset_index(drop=True), df2.reset_index(drop=True)], axis= 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLTEIcjzEB5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "4f534652-1f58-41e9-c347-459ca5f233b7"
      },
      "source": [
        "# Our first dataset is now stored in a Pandas Dataframe\n",
        "#df[[x for x in range(300, row_size)]] = pd.DataFrame([[0 for x in range(300, row_size)]], index=df.index)\n",
        "#for item in range(300,row_size):\n",
        "#  df[item]=0\n",
        "#  print(str(item)+\"/\"+str(row_size))\n",
        "#Here we check the schema, and its length\n",
        "#print(\"Schema: \"+str(df.columns))\n",
        "#print(\"Number of rows: \"+str(len(df)))\n",
        "print(df.head())\n",
        "\n",
        "df=df.rename(columns={'model_binary':str(300),'brand_binary':str(301)})\n",
        "\n",
        "print(df.head())\n",
        "df.columns = df.columns.map(str)\n",
        "ground_truth= pd.read_csv(location+\"final_extended_groundtruth.csv\", header=0)\n",
        "match_pair=dict()\n",
        "non_match_pair=dict()\n",
        "print(df['300'])\n",
        "emb_dict={}\n",
        "for index, row in df.iterrows():\n",
        "  emb_dict[row[\"spec_id\"].replace(\"'\",\"\")]=[row[str(item)] for item in range(0,302)]\n",
        "print(tuple(emb_dict.keys())[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         spec_id         0         1  ...       299             300         301\n",
            "0  buy.net//4233  0.031221  0.006270  ... -0.006325  00111010100101  0011111000\n",
            "1  buy.net//4236  0.005103  0.029665  ... -0.008783  00101111100110  0100010011\n",
            "2  buy.net//4239  0.005655  0.043519  ... -0.006725  00110000001010  0000110101\n",
            "3  buy.net//4247 -0.000266  0.020450  ...  0.037856  00000110000110  0000110101\n",
            "4  buy.net//4255  0.016109  0.042931  ...  0.041887  01000101101011  0001010000\n",
            "\n",
            "[5 rows x 303 columns]\n",
            "         spec_id         0         1  ...       299             300         301\n",
            "0  buy.net//4233  0.031221  0.006270  ... -0.006325  00111010100101  0011111000\n",
            "1  buy.net//4236  0.005103  0.029665  ... -0.008783  00101111100110  0100010011\n",
            "2  buy.net//4239  0.005655  0.043519  ... -0.006725  00110000001010  0000110101\n",
            "3  buy.net//4247 -0.000266  0.020450  ...  0.037856  00000110000110  0000110101\n",
            "4  buy.net//4255  0.016109  0.042931  ...  0.041887  01000101101011  0001010000\n",
            "\n",
            "[5 rows x 303 columns]\n",
            "0        00111010100101\n",
            "1        00101111100110\n",
            "2        00110000001010\n",
            "3        00000110000110\n",
            "4        01000101101011\n",
            "              ...      \n",
            "26674    00110000000111\n",
            "26675    00000101000000\n",
            "26676    00111110111111\n",
            "26677    00101011110111\n",
            "26678    01010000011010\n",
            "Name: 300, Length: 26679, dtype: object\n",
            "buy.net//4233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsEUVJnzEJEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, row in ground_truth.iterrows():\n",
        "  if row['label']==1:\n",
        "    if not row['left_spec_id'] in match_pair:\n",
        "      match_pair[row['left_spec_id']]=set()\n",
        "    match_pair[row['left_spec_id']].add(row['right_spec_id'])\n",
        "    if not row['right_spec_id'] in match_pair:\n",
        "      match_pair[row['right_spec_id']]=set()\n",
        "    match_pair[row['right_spec_id']].add(row['left_spec_id'])\n",
        "  else:\n",
        "    if not row['left_spec_id'] in non_match_pair:\n",
        "      non_match_pair[row['left_spec_id']]=set()\n",
        "    non_match_pair[row['left_spec_id']].add(row['right_spec_id'])\n",
        "    if not row['right_spec_id'] in non_match_pair:\n",
        "      non_match_pair[row['right_spec_id']]=set()\n",
        "    non_match_pair[row['right_spec_id']].add(row['left_spec_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9dnfd6EENe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_boundary=33 #Write 33 for pickign 33% of the data for testing and the remaining for training.\n",
        "test_set_matches=set()\n",
        "test_set_non_matches=set()\n",
        "for first_item in match_pair.keys():\n",
        "  for second_item in match_pair[first_item]:\n",
        "    if first_item < second_item:#Our tuple formation criteria\n",
        "      if random.randrange(1,101) <=split_boundary:\n",
        "        test_set_matches.add((first_item,second_item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOvjNQE1EYGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for first_item in non_match_pair.keys():\n",
        "  for second_item in non_match_pair[first_item]:\n",
        "    if first_item < second_item:#Our tuple formation criteria\n",
        "      if random.randrange(1,101) <=split_boundary:\n",
        "        test_set_non_matches.add((first_item,second_item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5OO-C_iEdmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def emb_to_array(emb):\n",
        "  arr=[float(emb[x]) for x in range(0,300)]\n",
        "  arr2=[0.0 for x in range(0,8192)] \n",
        "  #print(emb[300])\n",
        "  val=float(str(emb[300]))\n",
        "  if not math.isnan(val):\n",
        "    arr2[int(str(emb[300]),2)]=1.0\n",
        "  arr3=[0.0 for x in range(0,512)] \n",
        "  val=float(str(emb[301]))\n",
        "  #print(str(emb[301]))\n",
        "  if not math.isnan(val):\n",
        "    arr3[int(str(emb[301]),2)]=1.0\n",
        "  return arr+arr2+arr3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO3HF7BIEerX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deep_hash_network(code_length, network_type, input):\n",
        "    net = tf.nn.dropout(slim.fully_connected(input, 128, activation_fn=tf.nn.relu), external_dropout)\n",
        "    net = tf.nn.dropout(slim.fully_connected(net, 128, activation_fn=tf.nn.relu), internal_dropout)#, weights_regularizer=slim.l2_regularizer(1e-8))\n",
        "    #net = tf.nn.dropout(slim.fully_connected(net, int(code_length/2), activation_fn=tf.nn.relu), internal_dropout)#, weights_regularizer=slim.l2_regularizer(1e-8))  \n",
        "    #net = slim.fully_connected(net, 32, activation_fn=tf.nn.relu)#, weights_regularizer=slim.l2_regularizer(1e-8))  \n",
        "    hash_code = tf.nn.dropout(slim.fully_connected(net, code_length, activation_fn=None), 1.0)#  <- why?\n",
        "    return network_type(hash_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjxUMoMCEhLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "75e733f1-4242-4abe-f591-8c53b1b2c940"
      },
      "source": [
        "our_net=deep_hash_network\n",
        "print(\"Network function defined\")\n",
        "\n",
        "tf_device='/gpu:*'\n",
        "print(\"Device defined\")\n",
        "    \n",
        "shape=(1,row_size) \n",
        "print(\"I/O shapes defined\")\n",
        "    \n",
        "def _network_template(state):# <-Purpose?\n",
        "    return our_net(code_length, collections.namedtuple('DQH_network', ['hash_values']), state)\n",
        "print(\"Network wrapper function defined\")\n",
        "    \n",
        "batch_outputs1=[]# match\n",
        "batch_outputs2=[]# match\n",
        "batch_outputs3=[]# non_match"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network function defined\n",
            "Device defined\n",
            "I/O shapes defined\n",
            "Network wrapper function defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_94Rby_dElfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d95e8a4f-9d09-41bf-9b11-4b301e39ad58"
      },
      "source": [
        "def _build_network():\n",
        "    global batch_outputs1, batch_outputs2, batch_outputs3\n",
        "    net= tf.make_template('network', _network_template)\n",
        "    batch_outputs1=tf.clip_by_value(net(states1_ph),-1.,1.)#<-setting min and max value?\n",
        "    batch_outputs2=tf.clip_by_value(net(states2_ph),-1.,1.)\n",
        "    batch_outputs3=tf.clip_by_value(net(states3_ph),-1.,1.)      \n",
        "print(\"Network forward pass function defined\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network forward pass function defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4YX0ZdPEqYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50e269ca-07a6-4874-a96a-450f07ddd8e6"
      },
      "source": [
        "#RMSPRop\n",
        "def _build_train_op(): \n",
        "  #This defines our training operation, based on: Li, Wu-Jun, Sheng Wang, and Wang-Cheng Kang. \n",
        "  #\"Feature learning based deep supervised hashing with pairwise labels.\" \n",
        "  #arXiv preprint arXiv:1511.03855 (2015). However we extend it to a triple, because it worked better for us.\n",
        "  theta=tf.divide(tf.reduce_sum(tf.multiply(batch_outputs1[0],batch_outputs2[0]),1),2)\n",
        "  theta2=tf.divide(tf.reduce_sum(tf.multiply(batch_outputs1[0],batch_outputs3[0]),1),2)\n",
        "  theta5=tf.divide(tf.reduce_sum(tf.multiply(batch_outputs2[0],batch_outputs3[0]),1),2)\n",
        "  \n",
        "  sim_loss=-(\n",
        "      -tf.math.log(1+tf.math.exp(theta))+similarity_weight[0]*theta \n",
        "      \n",
        "  )\n",
        "\n",
        "  disim_loss2=-(\n",
        "      -tf.math.log(1+tf.math.exp(theta2))\n",
        "      \n",
        "  )\n",
        "\n",
        "  disim_loss5=-(\n",
        "      -tf.math.log(1+tf.math.exp(theta5))\n",
        "      \n",
        "  )\n",
        "  loss=sim_loss+disim_loss2+disim_loss5\n",
        "  print_op=tf.print(tf.reduce_mean(loss, axis=0))\n",
        "  with tf.control_dependencies([]):#Write print_op in there to print loss\n",
        "    gvs = optimizer.compute_gradients(loss)\n",
        "    capped_gvs = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs] #We clip by value to avoid exploiding gradients -10 to 10\n",
        "    return optimizer.apply_gradients(capped_gvs)\n",
        "print(\"Network backprop pass function (loss definition and minimization criteria) defined\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network backprop pass function (loss definition and minimization criteria) defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wkh_T_EE61N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "299bb57e-1709-41d5-94b8-97ea83b15a67"
      },
      "source": [
        "size_of_embedding=row_size\n",
        "with tf.device(tf_device):\n",
        "    batch_outputs1=tf.placeholder(tf.float32, name='bo1_ph')\n",
        "    batch_outputs2=tf.placeholder(tf.float32, name='bo2_ph')\n",
        "    batch_outputs3=tf.placeholder(tf.float32, name='bo3_ph')\n",
        "    similarity_weight=tf.placeholder(tf.float32, name='sw_ph')\n",
        "    states1_ph = tf.placeholder(tf.float32, (None,size_of_embedding), name='state1_ph')  \n",
        "    states2_ph = tf.placeholder(tf.float32, (None,size_of_embedding), name='state2_ph')\n",
        "    states3_ph = tf.placeholder(tf.float32, (None,size_of_embedding), name='state3_ph')\n",
        "    net= _build_network()\n",
        "    _train_op = _build_train_op()    \n",
        "print(\"Device selected and variables initialized\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-11-c7510ad80692>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Device selected and variables initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7crFJTs4FASF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "004c29b6-8722-4b47-99f7-b009810c5e03"
      },
      "source": [
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "_sess = tf.Session('', config=config)\n",
        "init_op = tf.initialize_all_variables()\n",
        "_sess.run(init_op)\n",
        "print(\"Tensorflow session initalized\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Tensorflow session initalized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyO8eeJvFXgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "a52aab88-671b-43f9-9ce0-63fed3bd4d98"
      },
      "source": [
        "coverage=0\n",
        "choices_selected=dict()#UPDATE LOGIC\n",
        "for item in list(match_pair.keys()):\n",
        "    choices_selected[item]=1\n",
        "sumcs=0\n",
        "for item in choices_selected.keys():#PUT THIS ALSO DOWN\n",
        "    sumcs+=choices_selected[item]\n",
        "probs=[choices_selected[key]/sumcs for key in list(match_pair.keys())]\n",
        "\n",
        "for num in range(0,total_it):\n",
        "    #print(num)\n",
        "    if use_probs:\n",
        "        KeysSelected = random.choices(list(match_pair.keys()), k=batch_size, weights=probs)\n",
        "        weights=[]\n",
        "        for a in KeysSelected:\n",
        "            mplist=[choices_selected[c] for c in list(match_pair[a])]\n",
        "            sum_mplist=sum(mplist)\n",
        "            mplist=[c/sum_mplist for c in mplist]\n",
        "            weights.append(mplist)\n",
        "        MatchesSelected = [random.choices(list(match_pair[KeysSelected[a]]), k=1, weights=weights[a])[0].replace(\"'\",\"\") for a in range(0,len(KeysSelected))]\n",
        "    else:\n",
        "        KeysSelected = random.choices(list(match_pair.keys()), k=batch_size)\n",
        "        MatchesSelected = [random.choice(list(match_pair[KeysSelected[a]])).replace(\"'\",\"\") for a in range(0,len(KeysSelected))]\n",
        "    firsts=[]\n",
        "    seconds=[]\n",
        "    non_matches=[]\n",
        "    for i in range(0,batch_size):\n",
        "        small_key=None\n",
        "        big_key=None\n",
        "        if KeysSelected[i]<=MatchesSelected[i]:\n",
        "          small_key=KeysSelected[i]\n",
        "          big_key=MatchesSelected[i]\n",
        "        else:\n",
        "          big_key=KeysSelected[i]\n",
        "          small_key=MatchesSelected[i]\n",
        "        if (KeysSelected[i] in non_match_pair) and (KeysSelected[i] in emb_dict) and (MatchesSelected[i] in emb_dict) and not (small_key,big_key) in test_set_matches:\n",
        "            we_selected= random.choice(tuple(non_match_pair[KeysSelected[i]])).replace(\"'\",\"\")\n",
        "            #if not we_selected in emb_dict:\n",
        "            #    we_selected=\"'\"+we_selected+\"'\"\n",
        "            if small_key<=we_selected:\n",
        "              big_key=we_selected\n",
        "            else:\n",
        "              big_key=small_key\n",
        "              small_key=we_selected\n",
        "            if we_selected in emb_dict and not (small_key, big_key) in test_set_non_matches:    \n",
        "              #print(\"we selected\"+we_selected)\n",
        "              #print(emb_dict[we_selected])\n",
        "              non_matches.append(emb_to_array(emb_dict[we_selected]))\n",
        "              firsts.append(emb_to_array(emb_dict[KeysSelected[i]]))\n",
        "              seconds.append(emb_to_array(emb_dict[MatchesSelected[i]])) \n",
        "    #print(len(firsts))\n",
        "    if (len(firsts)==0):\n",
        "      print(\"No item in match and embeddings\") \n",
        "    else:\n",
        "        val=[]\n",
        "        if coverage>=100:#num>total_it/4:\n",
        "            val=[1.05]#+(1.0-1*(((total_it)-num)/(total_it)))]      \n",
        "        elif coverage> 95:\n",
        "            val=[1.05]#Was 05, 15, 18\n",
        "        elif coverage> 90:\n",
        "            val=[1.1]\n",
        "        else:\n",
        "            val=[1.1]\n",
        "        for epoch in range(0,num_epochs):\n",
        "            [result]=_sess.run([_train_op], feed_dict={states1_ph: np.array(firsts,dtype=np.float64), states2_ph: np.array(seconds,dtype=np.float64), states3_ph: np.array(non_matches,dtype=np.float64), similarity_weight: np.array(val, dtype=np.float64)})\n",
        "        #print(num)\n",
        "        if num%report_interval==999:\n",
        "          test_set=[]\n",
        "          keys=list(emb_dict.keys())\n",
        "          for item in keys:\n",
        "             test_set.append(emb_to_array(emb_dict[item]))\n",
        "          b1= np.sign(_sess.run(batch_outputs1, {states1_ph: np.array(np.array(test_set,dtype=np.float64)), similarity_weight: np.array([1.0], dtype=np.float64)})[0])\n",
        "          hash_code_dict={}\n",
        "          key_2_hash_code_dict={}\n",
        "          for item in range(0,len(keys)):\n",
        "            hash_code=\"\".join([str(int(a)) for a in b1[item]])\n",
        "            hash_code=hash_code.replace(\"-1\",\"0\")\n",
        "            if not hash_code in hash_code_dict:\n",
        "              hash_code_dict[hash_code]=set()\n",
        "            set_to_use=hash_code_dict[hash_code]\n",
        "            set_to_use.add(keys[item].replace(\"'\",\"\"))\n",
        "            hash_code_dict[hash_code]=set_to_use\n",
        "            key_2_hash_code_dict[keys[item].replace(\"'\",\"\")]=hash_code\n",
        "          #print(len(key_2_hash_code_dict.keys()))\n",
        "          print(\"Iteration: \"+str(num))\n",
        "          print(\"Number of hash codes: \"+str(len(hash_code_dict.keys())))\n",
        "          total_matches=0\n",
        "          good_matches=0\n",
        "          faulty_items=set()\n",
        "          now=True\n",
        "          if use_probs:\n",
        "            choices_selected=dict()#UPDATE LOGIC\n",
        "            for item in list(match_pair.keys()):\n",
        "              choices_selected[item]=1\n",
        "          for k in match_pair.keys():\n",
        "            if not k in key_2_hash_code_dict:\n",
        "              if not k in faulty_items:\n",
        "                faulty_items.add(k)\n",
        "                print(\"Adding \"+k+\", from key_2_hash_code\")\n",
        "            else:\n",
        "              kscode= key_2_hash_code_dict[k]\n",
        "              for item in list(match_pair[k]):            \n",
        "                if not item in key_2_hash_code_dict:\n",
        "                  if not item in faulty_items:\n",
        "                    faulty_items.add(item)\n",
        "                else:\n",
        "                  total_matches+=1\n",
        "                  if kscode == key_2_hash_code_dict[item]:\n",
        "                    good_matches+=1\n",
        "                  elif use_probs:\n",
        "                    choices_selected[item]+=1\n",
        "                    choices_selected[k]+=1\n",
        "          if use_probs:\n",
        "            sumcs=0\n",
        "            for item in choices_selected.keys():#PUT THIS ALSO DOWN\n",
        "              sumcs+=choices_selected[item]\n",
        "            probs=[choices_selected[key]/sumcs for key in list(match_pair.keys())]\n",
        "          coverage=100*good_matches/total_matches\n",
        "          print(\"Coverage is: \"+str(coverage))\n",
        "          print(\"Non products keys: \"+str(len(faulty_items)))\n",
        "          computation=0\n",
        "          for key in hash_code_dict:\n",
        "            if len(hash_code_dict[key])>1:\n",
        "              computation+=(len(hash_code_dict[key])*len(hash_code_dict[key]))\n",
        "          computation=100*computation/(len(key_2_hash_code_dict.keys())*len(key_2_hash_code_dict.keys()))\n",
        "          print(\"Computation is \"+str(computation))\n",
        "          tensorboard.log_summary(\n",
        "            len(hash_code_dict.keys()), coverage, computation, num)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "dateTimeObj = datetime.now().replace(second=0, microsecond=0)\n",
        "f = open(location+str(dateTimeObj)+\"_Codes_\"+str(len(hash_code_dict.keys()))+\"_Cov_\"+str(coverage)+\"_Comp_\"+\"%.2f\" % computation+descriptor+\".csv\",'w')\n",
        "for item in key_2_hash_code_dict.keys():\n",
        "  f.write(item+\",\"+key_2_hash_code_dict[item]+\"\\n\")\n",
        "f.close()\n",
        "\n",
        "counter=0\n",
        "for item in sorted(list(faulty_items)):\n",
        "  print(str(counter)+\"-\"+item)\n",
        "  counter+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 999\n",
            "Number of hash codes: 1994\n",
            "Coverage is: 92.10125752289066\n",
            "Non products keys: 0\n",
            "Computation is 5.162397615436606\n",
            "Iteration: 1999\n",
            "Number of hash codes: 832\n",
            "Coverage is: 56.323911668969394\n",
            "Non products keys: 0\n",
            "Computation is 8.142728000444178\n",
            "Iteration: 2999\n",
            "Number of hash codes: 574\n",
            "Coverage is: 86.44357539282954\n",
            "Non products keys: 0\n",
            "Computation is 5.381768072713913\n",
            "Iteration: 3999\n",
            "Number of hash codes: 818\n",
            "Coverage is: 81.97316347797579\n",
            "Non products keys: 0\n",
            "Computation is 43.2317266240862\n",
            "Iteration: 5999\n",
            "Number of hash codes: 1134\n",
            "Coverage is: 74.47251949511744\n",
            "Non products keys: 0\n",
            "Computation is 23.201491282619582\n",
            "Iteration: 6999\n",
            "Number of hash codes: 892\n",
            "Coverage is: 75.44903168395663\n",
            "Non products keys: 0\n",
            "Computation is 18.15939814106076\n",
            "Iteration: 7999\n",
            "Number of hash codes: 1224\n",
            "Coverage is: 71.02077137437651\n",
            "Non products keys: 0\n",
            "Computation is 15.734729884100144\n",
            "Iteration: 8999\n",
            "Number of hash codes: 1301\n",
            "Coverage is: 96.04945788352106\n",
            "Non products keys: 0\n",
            "Computation is 36.07076329693862\n",
            "Iteration: 9999\n",
            "Number of hash codes: 1706\n",
            "Coverage is: 91.94904339273587\n",
            "Non products keys: 0\n",
            "Computation is 8.80212588510154\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}